{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Before-your-start:\" data-toc-modified-id=\"Before-your-start:-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Before your start:</a></span></li><li><span><a href=\"#Challenge-1---Explore-The-Dataset\" data-toc-modified-id=\"Challenge-1---Explore-The-Dataset-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Challenge 1 - Explore The Dataset</a></span></li><li><span><a href=\"#Desafío-1---Explorar-el-conjunto-de-datos\" data-toc-modified-id=\"Desafío-1---Explorar-el-conjunto-de-datos-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Desafío 1 - Explorar el conjunto de datos</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Explore-the-data-from-an-bird's-eye-view.\" data-toc-modified-id=\"Explore-the-data-from-an-bird's-eye-view.-3.0.0.1\"><span class=\"toc-item-num\">3.0.0.1&nbsp;&nbsp;</span>Explore the data from an bird's-eye view.</a></span></li><li><span><a href=\"#Explore-los-datos-a-vista-de-pájaro.\" data-toc-modified-id=\"Explore-los-datos-a-vista-de-pájaro.-3.0.0.2\"><span class=\"toc-item-num\">3.0.0.2&nbsp;&nbsp;</span>Explore los datos a vista de pájaro.</a></span></li><li><span><a href=\"#Next,-evaluate-if-the-columns-in-this-dataset-are-strongly-correlated.\" data-toc-modified-id=\"Next,-evaluate-if-the-columns-in-this-dataset-are-strongly-correlated.-3.0.0.3\"><span class=\"toc-item-num\">3.0.0.3&nbsp;&nbsp;</span>Next, evaluate if the columns in this dataset are strongly correlated.</a></span></li><li><span><a href=\"#A-continuación,-evalúe-si-las-columnas-de-este-conjunto-de-datos-están-fuertemente-correlacionadas.\" data-toc-modified-id=\"A-continuación,-evalúe-si-las-columnas-de-este-conjunto-de-datos-están-fuertemente-correlacionadas.-3.0.0.4\"><span class=\"toc-item-num\">3.0.0.4&nbsp;&nbsp;</span>A continuación, evalúe si las columnas de este conjunto de datos están fuertemente correlacionadas.</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Challenge-2---Remove-Column-Collinearity.\" data-toc-modified-id=\"Challenge-2---Remove-Column-Collinearity.-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Challenge 2 - Remove Column Collinearity.</a></span></li><li><span><a href=\"#Challenge-3---Handle-Missing-Values\" data-toc-modified-id=\"Challenge-3---Handle-Missing-Values-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Challenge 3 - Handle Missing Values</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#In-the-cells-below,-handle-the-missing-values-from-the-dataset.-Remember-to-comment-the-rationale-of-your-decisions.\" data-toc-modified-id=\"In-the-cells-below,-handle-the-missing-values-from-the-dataset.-Remember-to-comment-the-rationale-of-your-decisions.-5.0.0.1\"><span class=\"toc-item-num\">5.0.0.1&nbsp;&nbsp;</span>In the cells below, handle the missing values from the dataset. Remember to comment the rationale of your decisions.</a></span></li><li><span><a href=\"#Again,-examine-the-number-of-missing-values-in-each-column.\" data-toc-modified-id=\"Again,-examine-the-number-of-missing-values-in-each-column.-5.0.0.2\"><span class=\"toc-item-num\">5.0.0.2&nbsp;&nbsp;</span>Again, examine the number of missing values in each column.</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Challenge-4---Handle-WHOIS_*-Categorical-Data\" data-toc-modified-id=\"Challenge-4---Handle-WHOIS_*-Categorical-Data-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Challenge 4 - Handle <code>WHOIS_*</code> Categorical Data</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#In-the-cells-below,-fix-the-country-values-as-intructed-above.\" data-toc-modified-id=\"In-the-cells-below,-fix-the-country-values-as-intructed-above.-6.0.0.1\"><span class=\"toc-item-num\">6.0.0.1&nbsp;&nbsp;</span>In the cells below, fix the country values as intructed above.</a></span></li><li><span><a href=\"#If-a-limited-number-of-values-account-for-the-majority-of-data,-we-can-retain-these-top-values-and-re-label-all-other-rare-values.\" data-toc-modified-id=\"If-a-limited-number-of-values-account-for-the-majority-of-data,-we-can-retain-these-top-values-and-re-label-all-other-rare-values.-6.0.0.2\"><span class=\"toc-item-num\">6.0.0.2&nbsp;&nbsp;</span>If a limited number of values account for the majority of data, we can retain these top values and re-label all other rare values.</a></span></li><li><span><a href=\"#Si-una-cantidad-limitada-de-valores-representa-la-mayoría-de-los-datos,-podemos-retener-estos-valores-principales-y-volver-a-etiquetar-todos-los-demás-valores-raros.\" data-toc-modified-id=\"Si-una-cantidad-limitada-de-valores-representa-la-mayoría-de-los-datos,-podemos-retener-estos-valores-principales-y-volver-a-etiquetar-todos-los-demás-valores-raros.-6.0.0.3\"><span class=\"toc-item-num\">6.0.0.3&nbsp;&nbsp;</span>Si una cantidad limitada de valores representa la mayoría de los datos, podemos retener estos valores principales y volver a etiquetar todos los demás valores raros.</a></span></li><li><span><a href=\"#After-verifying,-now-let's-keep-the-top-10-values-of-the-column-and-re-label-other-columns-with-OTHER.\" data-toc-modified-id=\"After-verifying,-now-let's-keep-the-top-10-values-of-the-column-and-re-label-other-columns-with-OTHER.-6.0.0.4\"><span class=\"toc-item-num\">6.0.0.4&nbsp;&nbsp;</span>After verifying, now let's keep the top 10 values of the column and re-label other columns with <code>OTHER</code>.</a></span></li><li><span><a href=\"#In-the-next-cell,-drop-['WHOIS_STATEPRO',-'WHOIS_REGDATE',-'WHOIS_UPDATED_DATE'].\" data-toc-modified-id=\"In-the-next-cell,-drop-['WHOIS_STATEPRO',-'WHOIS_REGDATE',-'WHOIS_UPDATED_DATE'].-6.0.0.5\"><span class=\"toc-item-num\">6.0.0.5&nbsp;&nbsp;</span>In the next cell, drop <code>['WHOIS_STATEPRO', 'WHOIS_REGDATE', 'WHOIS_UPDATED_DATE']</code>.</a></span></li><li><span><a href=\"#En-la-siguiente-celda,-suelte-['WHOIS_STATEPRO',-'WHOIS_REGDATE',-'WHOIS_UPDATED_DATE'].\" data-toc-modified-id=\"En-la-siguiente-celda,-suelte-['WHOIS_STATEPRO',-'WHOIS_REGDATE',-'WHOIS_UPDATED_DATE'].-6.0.0.6\"><span class=\"toc-item-num\">6.0.0.6&nbsp;&nbsp;</span>En la siguiente celda, suelte <code>['WHOIS_STATEPRO', 'WHOIS_REGDATE', 'WHOIS_UPDATED_DATE']</code>.</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Challenge-5---Handle-Remaining-Categorical-Data-&amp;-Convert-to-Ordinal\" data-toc-modified-id=\"Challenge-5---Handle-Remaining-Categorical-Data-&amp;-Convert-to-Ordinal-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Challenge 5 - Handle Remaining Categorical Data &amp; Convert to Ordinal</a></span></li><li><span><a href=\"#Desafío-5:-manejar-los-datos-categóricos-restantes-y-convertirlos-a-ordinales\" data-toc-modified-id=\"Desafío-5:-manejar-los-datos-categóricos-restantes-y-convertirlos-a-ordinales-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Desafío 5: manejar los datos categóricos restantes y convertirlos a ordinales</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#URL-is-easy.-We'll-simply-drop-it-because-it-has-too-many-unique-values-that-there's-no-way-for-us-to-consolidate.\" data-toc-modified-id=\"URL-is-easy.-We'll-simply-drop-it-because-it-has-too-many-unique-values-that-there's-no-way-for-us-to-consolidate.-8.0.0.1\"><span class=\"toc-item-num\">8.0.0.1&nbsp;&nbsp;</span><code>URL</code> is easy. We'll simply drop it because it has too many unique values that there's no way for us to consolidate.</a></span></li><li><span><a href=\"#URL-es-fácil.-Simplemente-lo-descartaremos-porque-tiene-demasiados-valores-únicos-que-no-hay-manera-de-consolidar.\" data-toc-modified-id=\"URL-es-fácil.-Simplemente-lo-descartaremos-porque-tiene-demasiados-valores-únicos-que-no-hay-manera-de-consolidar.-8.0.0.2\"><span class=\"toc-item-num\">8.0.0.2&nbsp;&nbsp;</span><code>URL</code> es fácil. Simplemente lo descartaremos porque tiene demasiados valores únicos que no hay manera de consolidar.</a></span></li><li><span><a href=\"#Print-the-unique-value-counts-of-CHARSET.-You-see-there-are-only-a-few-unique-values.-So-we-can-keep-it-as-it-is.\" data-toc-modified-id=\"Print-the-unique-value-counts-of-CHARSET.-You-see-there-are-only-a-few-unique-values.-So-we-can-keep-it-as-it-is.-8.0.0.3\"><span class=\"toc-item-num\">8.0.0.3&nbsp;&nbsp;</span>Print the unique value counts of <code>CHARSET</code>. You see there are only a few unique values. So we can keep it as it is.</a></span></li><li><span><a href=\"#Imprime-los-recuentos-de-valores-únicos-de-CHARSET.-Verá-que-solo-hay-unos-pocos-valores-únicos.-Entonces-podemos-mantenerlo-como-está.\" data-toc-modified-id=\"Imprime-los-recuentos-de-valores-únicos-de-CHARSET.-Verá-que-solo-hay-unos-pocos-valores-únicos.-Entonces-podemos-mantenerlo-como-está.-8.0.0.4\"><span class=\"toc-item-num\">8.0.0.4&nbsp;&nbsp;</span>Imprime los recuentos de valores únicos de <code>CHARSET</code>. Verá que solo hay unos pocos valores únicos. Entonces podemos mantenerlo como está.</a></span></li><li><span><a href=\"#Before-you-think-of-your-own-solution,-don't-read-the-instructions-that-come-next.\" data-toc-modified-id=\"Before-you-think-of-your-own-solution,-don't-read-the-instructions-that-come-next.-8.0.0.5\"><span class=\"toc-item-num\">8.0.0.5&nbsp;&nbsp;</span>Before you think of your own solution, don't read the instructions that come next.</a></span></li><li><span><a href=\"#Antes-de-pensar-en-su-propia-solución,-no-lea-las-instrucciones-que-vienen-a-continuación.\" data-toc-modified-id=\"Antes-de-pensar-en-su-propia-solución,-no-lea-las-instrucciones-que-vienen-a-continuación.-8.0.0.6\"><span class=\"toc-item-num\">8.0.0.6&nbsp;&nbsp;</span>Antes de pensar en su propia solución, no lea las instrucciones que vienen a continuación.</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Challenge-6---Modeling,-Prediction,-and-Evaluation\" data-toc-modified-id=\"Challenge-6---Modeling,-Prediction,-and-Evaluation-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Challenge 6 - Modeling, Prediction, and Evaluation</a></span></li><li><span><a href=\"#Desafío-6---Modelado,-predicción-y-evaluación\" data-toc-modified-id=\"Desafío-6---Modelado,-predicción-y-evaluación-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Desafío 6 - Modelado, predicción y evaluación</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#In-this-lab,-we-will-try-two-different-models-and-compare-our-results.\" data-toc-modified-id=\"In-this-lab,-we-will-try-two-different-models-and-compare-our-results.-10.0.0.1\"><span class=\"toc-item-num\">10.0.0.1&nbsp;&nbsp;</span>In this lab, we will try two different models and compare our results.</a></span></li><li><span><a href=\"#Our-second-algorithm-is-is-DecisionTreeClassifier\" data-toc-modified-id=\"Our-second-algorithm-is-is-DecisionTreeClassifier-10.0.0.2\"><span class=\"toc-item-num\">10.0.0.2&nbsp;&nbsp;</span>Our second algorithm is is DecisionTreeClassifier</a></span></li><li><span><a href=\"#Nuestro-segundo-algoritmo-es-DecisionTreeClassifier\" data-toc-modified-id=\"Nuestro-segundo-algoritmo-es-DecisionTreeClassifier-10.0.0.3\"><span class=\"toc-item-num\">10.0.0.3&nbsp;&nbsp;</span>Nuestro segundo algoritmo es DecisionTreeClassifier</a></span></li><li><span><a href=\"#We'll-create-another-DecisionTreeClassifier-model-with-max_depth=5.\" data-toc-modified-id=\"We'll-create-another-DecisionTreeClassifier-model-with-max_depth=5.-10.0.0.4\"><span class=\"toc-item-num\">10.0.0.4&nbsp;&nbsp;</span>We'll create another DecisionTreeClassifier model with max_depth=5.</a></span></li><li><span><a href=\"#Crearemos-otro-modelo-de-DecisionTreeClassifier-con-max_depth=5.\" data-toc-modified-id=\"Crearemos-otro-modelo-de-DecisionTreeClassifier-con-max_depth=5.-10.0.0.5\"><span class=\"toc-item-num\">10.0.0.5&nbsp;&nbsp;</span>Crearemos otro modelo de DecisionTreeClassifier con max_depth=5.</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Bonus-Challenge---Feature-Scaling\" data-toc-modified-id=\"Bonus-Challenge---Feature-Scaling-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Bonus Challenge - Feature Scaling</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before your start:\n",
    "- Read the README.md file\n",
    "- Comment as much as you can and use the resources in the README.md file\n",
    "- Happy learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your libraries:\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pylab as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "In this lab, we will explore a dataset that describes websites with different features and labels them either benign or malicious . We will use supervised learning algorithms to figure out what feature patterns malicious websites are likely to have and use our model to predict malicious websites.\n",
    "\n",
    "Your features will be:\n",
    "\n",
    "+ URL: it is the anonymous identification of the URL analyzed in the study\n",
    "+ URL_LENGTH: it is the number of characters in the URL\n",
    "+ NUMBER_SPECIAL_CHARACTERS: it is number of special characters identified in the URL, such as, “/”, “%”, “#”, “&”, “. “, “=”\n",
    "+ CHARSET: it is a categorical value and its meaning is the character encoding standard (also called character set).\n",
    "+ SERVER: it is a categorical value and its meaning is the operative system of the server got from the packet response.\n",
    "+ CONTENT_LENGTH: it represents the content size of the HTTP header.\n",
    "+ WHOIS_COUNTRY: it is a categorical variable, its values are the countries we got from the server response (specifically, our script used the API of Whois).\n",
    "+ WHOIS_STATEPRO: it is a categorical variable, its values are the states we got from the server response (specifically, our script used the API of Whois).\n",
    "+ WHOIS_REGDATE: Whois provides the server registration date, so, this variable has date values with format DD/MM/YYY HH:MM\n",
    "+ WHOIS_UPDATED_DATE: Through the Whois we got the last update date from the server analyzed\n",
    "+ TCP_CONVERSATION_EXCHANGE: This variable is the number of TCP packets exchanged between the server and our honeypot client\n",
    "+ DIST_REMOTE_TCP_PORT: it is the number of the ports detected and different to TCP\n",
    "+ REMOTE_IPS: this variable has the total number of IPs connected to the honeypot\n",
    "+ APP_BYTES: this is the number of bytes transfered\n",
    "+ SOURCE_APP_PACKETS: packets sent from the honeypot to the server\n",
    "+ REMOTE_APP_PACKETS: packets received from the server\n",
    "+ APP_PACKETS: this is the total number of IP packets generated during the communication between the honeypot and the server\n",
    "+ DNS_QUERY_TIMES: this is the number of DNS packets generated during the communication between the honeypot and the server\n",
    "+ TYPE: this is a categorical variable, its values represent the type of web page analyzed, specifically, 1 is for malicious websites and 0 is for benign websites\n",
    "\n",
    "# Challenge 1 - Explore The Dataset\n",
    "\n",
    "Let's start by exploring the dataset. First load the data file:\n",
    "\n",
    "\n",
    "*Source: [kaggle](https://www.kaggle.com/viratkothari/malicious-and-benign-websites-classification)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "En esta práctica de laboratorio, exploraremos un conjunto de datos que describe sitios web con diferentes características y los etiqueta como benignos o maliciosos. Usaremos algoritmos de aprendizaje supervisado para descubrir qué patrones de funciones es probable que tengan los sitios web maliciosos y usaremos nuestro modelo para predecir sitios web maliciosos.\n",
    "\n",
    "Tus características serán:\n",
    "\n",
    "+ URL: es la identificación anónima de la URL analizada en el estudio\n",
    "+ URL_LONGITUD: es el número de caracteres de la URL\n",
    "+ NÚMERO_CARACTERES_ESPECIALES: es el número de caracteres especiales identificados en la URL, tales como, “/”, “%”, “#”, “&”, “. “, “=”\n",
    "+ CHARSET: es un valor categórico y su significado es el estándar de codificación de caracteres (también llamado conjunto de caracteres).\n",
    "+ SERVIDOR: es un valor categórico y su significado es el sistema operativo del servidor obtenido de la respuesta del paquete.\n",
    "+ CONTENT_LENGTH: representa el tamaño del contenido de la cabecera HTTP.\n",
    "+ WHOIS_PAÍS: es una variable categórica, sus valores son los países que obtuvimos de la respuesta del servidor (específicamente, nuestro script usó la API de Whois).\n",
    "+ WHOIS_STATEPRO: es una variable categórica, sus valores son los estados que obtuvimos de la respuesta del servidor (específicamente, nuestro script usó la API de Whois).\n",
    "+ WHOIS_REGDATE: Whois proporciona la fecha de registro del servidor, por lo que esta variable tiene valores de fecha con formato DD/MM/YYY HH:MM\n",
    "+ WHOIS_ACTUALIZADO_FECHA: A través del Whois obtuvimos la última fecha de actualización del servidor analizado\n",
    "+ TCP_CONVERSACIÓN_INTERCAMBIO: Esta variable es el número de paquetes TCP intercambiados entre el servidor y nuestro cliente honeypot\n",
    "+ DIST_REMOTE_TCP_PORT: es el número de puertos detectados y diferentes a TCP\n",
    "+ REMOTE_IPS: esta variable tiene el número total de IPs conectadas al honeypot\n",
    "+ APP_BYTES: este es el número de bytes transferidos\n",
    "+ SOURCE_APP_PACKETS: paquetes enviados desde el honeypot al servidor\n",
    "+ REMOTE_APP_PACKETS: paquetes recibidos del servidor\n",
    "+ *APP_PACKETS*: este es el número total de paquetes IP generados durante la comunicación entre el honeypot y el servidor\n",
    "+ DNS_QUERY_TIMES: este es el número de paquetes DNS generados durante la comunicación entre el honeypot y el servidor\n",
    "+ TIPO: esta es una variable categórica, sus valores representan el tipo de página web analizada, específicamente, 1 es para sitios web maliciosos y 0 es para sitios web benignos\n",
    "\n",
    "# Desafío 1 - Explorar el conjunto de datos\n",
    "\n",
    "Comencemos explorando el conjunto de datos. Primero cargue el archivo de datos:\n",
    "\n",
    "\n",
    "*Fuente: [kaggle](https://www.kaggle.com/viratkothari/malicious-and-benign-websites-classification)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../website.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "#### Explore the data from an bird's-eye view.\n",
    "\n",
    "You should already been very familiar with the procedures now so we won't provide the instructions step by step. Reflect on what you did in the previous labs and explore the dataset.\n",
    "\n",
    "Things you'll be looking for:\n",
    "\n",
    "* What the dataset looks like?\n",
    "* What are the data types?\n",
    "* Which columns contain the features of the websites?\n",
    "* Which column contains the feature we will predict? What is the code standing for benign vs malicious websites?\n",
    "* Do we need to transform any of the columns from categorical to ordinal values? If so what are these columns?\n",
    "\n",
    "Feel free to add additional cells for your explorations. Make sure to comment what you find out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "#### Explore los datos a vista de pájaro.\n",
    "\n",
    "Ya debería estar muy familiarizado con los procedimientos ahora, por lo que no le proporcionaremos las instrucciones paso a paso. Reflexione sobre lo que hizo en los laboratorios anteriores y explore el conjunto de datos.\n",
    "\n",
    "Cosas que estarás buscando:\n",
    "\n",
    "*¿Cómo se ve el conjunto de datos?* ¿Cuáles son los tipos de datos?\n",
    "*¿Qué columnas contienen las características de los sitios web?* ¿Qué columna contiene la función que predeciremos? ¿Cuál es el código que representa los sitios web benignos frente a los maliciosos?\n",
    "* ¿Necesitamos transformar alguna de las columnas de valores categóricos a ordinales? Si es así, ¿cuáles son estas columnas?\n",
    "\n",
    "Siéntase libre de agregar celdas adicionales para sus exploraciones. Asegúrate de comentar lo que averigües."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "#### Next, evaluate if the columns in this dataset are strongly correlated.\n",
    "\n",
    "In the Mushroom supervised learning lab we did recently, we mentioned we are concerned if our dataset has strongly correlated columns because if it is the case we need to choose certain ML algorithms instead of others. We need to evaluate this for our dataset now.\n",
    "\n",
    "Luckily, most of the columns in this dataset are ordinal which makes things a lot easier for us. In the next cells below, evaluate the level of collinearity of the data.\n",
    "\n",
    "We provide some general directions for you to consult in order to complete this step:\n",
    "\n",
    "1. You will create a correlation matrix using the numeric columns in the dataset.\n",
    "\n",
    "1. Create a heatmap using `seaborn` to visualize which columns have high collinearity.\n",
    "\n",
    "1. Comment on which columns you might need to remove due to high collinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "#### A continuación, evalúe si las columnas de este conjunto de datos están fuertemente correlacionadas.\n",
    "\n",
    "En el laboratorio de aprendizaje supervisado de Mushroom que hicimos recientemente, mencionamos que nos preocupa si nuestro conjunto de datos tiene columnas fuertemente correlacionadas porque, si ese es el caso, debemos elegir ciertos algoritmos de ML en lugar de otros. Necesitamos evaluar esto para nuestro conjunto de datos ahora.\n",
    "\n",
    "Afortunadamente, la mayoría de las columnas en este conjunto de datos son ordinales, lo que nos facilita mucho las cosas. En las siguientes celdas a continuación, evalúe el nivel de colinealidad de los datos.\n",
    "\n",
    "Te proporcionamos algunas indicaciones generales para que las consultes a fin de completar este paso:\n",
    "\n",
    "1. Creará una matriz de correlación usando las columnas numéricas en el conjunto de datos.\n",
    "\n",
    "1. Cree un mapa de calor usando `seaborn` para visualizar qué columnas tienen alta colinealidad.\n",
    "\n",
    "1. Comenta qué columnas podrías necesitar eliminar debido a la alta colinealidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "sns.set(style='white')\n",
    "\n",
    "mask=np.triu(np.ones_like(df.corr(), dtype=bool))\n",
    "\n",
    "cmap=sns.diverging_palette(0, 10, as_cmap=True)\n",
    "\n",
    "\n",
    "sns.heatmap(df.corr(),\n",
    "           mask=mask,\n",
    "          cmap=cmap,\n",
    "          center=0,\n",
    "          square=True,\n",
    "          annot=True,\n",
    "          linewidths=0.5,\n",
    "          cbar_kws={'shrink': 0.5});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('CONTENT_LENGTH', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['TCP_CONVERSATION_EXCHANGE', 'SOURCE_APP_PACKETS', 'REMOTE_APP_PACKETS', 'SOURCE_APP_BYTES'], axis=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['APP_BYTES'], axis=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['URL_LENGTH'], axis=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.WHOIS_COUNTRY == \"None\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop(['APP_BYTES'], axis=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "# Challenge 2 - Remove Column Collinearity.\n",
    "\n",
    "From the heatmap you created, you should have seen at least 3 columns that can be removed due to high collinearity. Remove these columns from the dataset.\n",
    "\n",
    "Note that you should remove as few columns as you can. You don't have to remove all the columns at once. But instead, try removing one column, then produce the heatmap again to determine if additional columns should be removed. As long as the dataset no longer contains columns that are correlated for over 90%, you can stop. Also, keep in mind when two columns have high collinearity, you only need to remove one of them but not both.\n",
    "\n",
    "In the cells below, remove as few columns as you can to eliminate the high collinearity in the dataset. Make sure to comment on your way so that the instructional team can learn about your thinking process which allows them to give feedback. At the end, print the heatmap again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "# Challenge 3 - Handle Missing Values\n",
    "\n",
    "The next step would be handling missing values. **We start by examining the number of missing values in each column, which you will do in the next cell.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "If you remember in the previous labs, we drop a column if the column contains a high proportion of missing values. After dropping those problematic columns, we drop the rows with missing values.\n",
    "\n",
    "#### In the cells below, handle the missing values from the dataset. Remember to comment the rationale of your decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Again, examine the number of missing values in each column. \n",
    "\n",
    "If all cleaned, proceed. Otherwise, go back and do more cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 4 - Handle `WHOIS_*` Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several categorical columns we need to handle. These columns are:\n",
    "\n",
    "* `URL`\n",
    "* `CHARSET`\n",
    "* `SERVER`\n",
    "* `WHOIS_COUNTRY`\n",
    "* `WHOIS_STATEPRO`\n",
    "* `WHOIS_REGDATE`\n",
    "* `WHOIS_UPDATED_DATE`\n",
    "\n",
    "How to handle string columns is always case by case. Let's start by working on `WHOIS_COUNTRY`. Your steps are:\n",
    "\n",
    "1. List out the unique values of `WHOIS_COUNTRY`.\n",
    "1. Consolidate the country values with consistent country codes. For example, the following values refer to the same country and should use consistent country code:\n",
    "    * `CY` and `Cyprus`\n",
    "    * `US` and `us`\n",
    "    * `SE` and `se`\n",
    "    * `GB`, `United Kingdom`, and `[u'GB'; u'UK']`\n",
    "\n",
    "#### In the cells below, fix the country values as intructed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "dicc = {\"Cyprus\": 'CY', 'us':'US', 'se':'SE',\"u'GB\":'GB', \"u'UK\":'GB',\"United Kingdom\":'GB', \"[u'GB'; u'UK']\" :'GB', \"ru\":\"RU\"}\n",
    "\n",
    "df.WHOIS_COUNTRY = df.WHOIS_COUNTRY.replace(dicc)\n",
    "\n",
    "df.WHOIS_COUNTRY.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "Since we have fixed the country values, can we convert this column to ordinal now?\n",
    "\n",
    "Not yet. If you reflect on the previous labs how we handle categorical columns, you probably remember we ended up dropping a lot of those columns because there are too many unique values. Too many unique values in a column is not desirable in machine learning because it makes prediction inaccurate. But there are workarounds under certain conditions. One of the fixable conditions is:\n",
    "\n",
    "#### If a limited number of values account for the majority of data, we can retain these top values and re-label all other rare values.\n",
    "\n",
    "The `WHOIS_COUNTRY` column happens to be this case. You can verify it by print a bar chart of the `value_counts` in the next cell to verify:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Dado que hemos fijado los valores de los países, ¿podemos convertir esta columna en ordinal ahora?\n",
    "\n",
    "No todavía. Si reflexiona sobre cómo manejamos las columnas categóricas en los laboratorios anteriores, probablemente recuerde que terminamos descartando muchas de esas columnas porque hay demasiados valores únicos. Demasiados valores únicos en una columna no son deseables en el aprendizaje automático porque hace que la predicción sea imprecisa. Pero hay soluciones bajo ciertas condiciones. Una de las condiciones reparables es:\n",
    "\n",
    "#### Si una cantidad limitada de valores representa la mayoría de los datos, podemos retener estos valores principales y volver a etiquetar todos los demás valores raros.\n",
    "\n",
    "La columna `WHOIS_COUNTRY` es este caso. Puede verificarlo imprimiendo un gráfico de barras de `value_counts` en la siguiente celda para verificar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Your code here\n",
    "df2=df.WHOIS_COUNTRY.value_counts()\n",
    "plt.figure(figsize=(20,5))\n",
    "y_pos = np.arange(len(df2))\n",
    "# Create bars\n",
    "plt.bar(y_pos, df2)\n",
    "# Create names on the x-axis\n",
    "plt.xticks(y_pos, df2.index)\n",
    "plt.xticks(rotation = 90)\n",
    "# Show graphic\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After verifying, now let's keep the top 10 values of the column and re-label other columns with `OTHER`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['WHOIS_COUNTRY'] = df['WHOIS_COUNTRY'].apply(lambda x:'OTHER'if x not in df['WHOIS_COUNTRY'].value_counts()[:10].index else x)\n",
    "df['WHOIS_COUNTRY'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "Now since `WHOIS_COUNTRY` has been re-labelled, we don't need `WHOIS_STATEPRO` any more because the values of the states or provinces may not be relevant any more. We'll drop this column.\n",
    "\n",
    "In addition, we will also drop `WHOIS_REGDATE` and `WHOIS_UPDATED_DATE`. These are the registration and update dates of the website domains. Not of our concerns.\n",
    "\n",
    "#### In the next cell, drop `['WHOIS_STATEPRO', 'WHOIS_REGDATE', 'WHOIS_UPDATED_DATE']`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Ahora que `WHOIS_COUNTRY` ha sido reetiquetado, ya no necesitamos `WHOIS_STATEPRO` porque los valores de los estados o provincias ya no son relevantes. Dejaremos esta columna.\n",
    "\n",
    "Además, también eliminaremos `WHOIS_REGDATE` y `WHOIS_UPDATED_DATE`. Estas son las fechas de registro y actualización de los dominios del sitio web. No de nuestras preocupaciones.\n",
    "\n",
    "#### En la siguiente celda, suelte `['WHOIS_STATEPRO', 'WHOIS_REGDATE', 'WHOIS_UPDATED_DATE']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "df = df.drop(['WHOIS_STATEPRO', 'WHOIS_REGDATE', 'WHOIS_UPDATED_DATE'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "# Challenge 5 - Handle Remaining Categorical Data & Convert to Ordinal\n",
    "\n",
    "Now print the `dtypes` of the data again. Besides `WHOIS_COUNTRY` which we already fixed, there should be 3 categorical columns left: `URL`, `CHARSET`, and `SERVER`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "# Desafío 5: manejar los datos categóricos restantes y convertirlos a ordinales\n",
    "\n",
    "Ahora imprima los `dtypes` de los datos de nuevo. Además de `WHOIS_COUNTRY` que ya arreglamos, deberían quedar 3 columnas categóricas: `URL`, `CHARSET` y `SERVER`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "#### `URL` is easy. We'll simply drop it because it has too many unique values that there's no way for us to consolidate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "#### `URL` es fácil. Simplemente lo descartaremos porque tiene demasiados valores únicos que no hay manera de consolidar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "df = df.drop('URL', axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "#### Print the unique value counts of `CHARSET`. You see there are only a few unique values. So we can keep it as it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "#### Imprime los recuentos de valores únicos de `CHARSET`. Verá que solo hay unos pocos valores únicos. Entonces podemos mantenerlo como está."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.CHARSET.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicc2 = {\"iso-8859-1\": 'ISO-8859-1', 'utf-8':'UTF-8'}\n",
    "\n",
    "df.CHARSET = df.CHARSET.replace(dicc2)\n",
    "\n",
    "df.CHARSET.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "`SERVER` is a little more complicated. Print its unique values and think about how you can consolidate those values.\n",
    "\n",
    "#### Before you think of your own solution, don't read the instructions that come next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "`SERVER` es un poco más complicado. Imprime sus valores únicos y piensa en cómo puedes consolidar esos valores.\n",
    "\n",
    "#### Antes de pensar en su propia solución, no lea las instrucciones que vienen a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "df.SERVER.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Think Hard](../think-hard.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your comment here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there are so many unique values in the `SERVER` column, there are actually only 3 main server types: `Microsoft`, `Apache`, and `nginx`. Just check if each `SERVER` value contains any of those server types and re-label them. For `SERVER` values that don't contain any of those substrings, label with `Other`.\n",
    "\n",
    "At the end, your `SERVER` column should only contain 4 unique values: `Microsoft`, `Apache`, `nginx`, and `Other`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def name_server(x): \n",
    "    if 'Microsoft' in x:\n",
    "        x = 'Microsoft'\n",
    "    elif 'Apache' in x:\n",
    "        x = 'Apache'\n",
    "    elif 'nginx' in x:\n",
    "        x = 'nginx'\n",
    "    else:\n",
    "        x = 'Other'\n",
    "    return x.upper()    # x es una key del diccionario color, color[x] es el value\n",
    "\n",
    "#X.color=X.color.apply(cambio_color)\n",
    "\n",
    "df.SERVER =df.SERVER.apply(lambda x: name_server(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Count `SERVER` value counts here\n",
    "df.SERVER.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, all our categorical data are fixed now. **Let's convert them to ordinal data using Pandas' `get_dummies` function ([documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)).** Make sure you drop the categorical columns by passing `drop_first=True` to `get_dummies` as we don't need them any more. **Also, assign the data with dummy values to a new variable `website_dummy`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "df_dummy=pd.get_dummies(df, columns=['CHARSET', 'SERVER','WHOIS_COUNTRY'])   # drop_first=True, por defecto es False \n",
    "\n",
    "df_dummy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "Now, inspect `website_dummy` to make sure the data and types are intended - there shouldn't be any categorical columns at this point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Ahora, inspeccione `website_dummy` para asegurarse de que los datos y los tipos sean los previstos; no debería haber columnas categóricas en este punto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "df_dummy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "# Challenge 6 - Modeling, Prediction, and Evaluation\n",
    "\n",
    "We'll start off this section by splitting the data to train and test. **Name your 4 variables `X_train`, `X_test`, `y_train`, and `y_test`. Select 80% of the data for training and 20% for testing.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "# Desafío 6 - Modelado, predicción y evaluación\n",
    "\n",
    "Comenzaremos esta sección dividiendo los datos para entrenar y probar. **Nombra tus 4 variables `X_train`, `X_test`, `y_train` y `y_test`. Seleccione el 80 % de los datos para entrenamiento y el 20 % para pruebas.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df_dummy.drop('Type', axis=1)\n",
    "y = df.Type\n",
    "\n",
    "# Your code here:\n",
    "from sklearn.model_selection import train_test_split as tts \n",
    "\n",
    "X_train, X_test, y_train, y_test = tts(X, y, train_size=0.8, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this lab, we will try two different models and compare our results.\n",
    "\n",
    "The first model we will use in this lab is logistic regression. We have previously learned about logistic regression as a classification algorithm. In the cell below, load `LogisticRegression` from scikit-learn and initialize the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "Next, fit the model to our training data. We have already separated our data into 4 parts. Use those in your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "A continuación, ajuste el modelo a nuestros datos de entrenamiento. Ya hemos separado nuestros datos en 4 partes. Úselos en su modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "logreg=LogisticRegression(max_iter=1000)\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "finally, import `confusion_matrix` and `accuracy_score` from `sklearn.metrics` and fit our testing data. Assign the fitted data to `y_pred` and print the confusion matrix as well as the accuracy score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "finalmente, importa `confusion_matrix` y `accuracy_score` desde `sklearn.metrics` y ajusta nuestros datos de prueba. Asigne los datos ajustados a `y_pred` e imprima la matriz de confusión, así como la puntuación de precisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=logreg.predict(X_test)\n",
    "\n",
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob=logreg.predict_proba(X_test)\n",
    "\n",
    "y_prob[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_pred==y_test)/y_pred.shape[0] * 100   # acierto, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "What are your thoughts on the performance of the model? Write your conclusions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "¿Cuáles son sus pensamientos sobre el rendimiento del modelo? Escribe tus conclusiones a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your conclusions here:\n",
    "''' el 92% de los sitios web analizados son benignos'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "#### Our second algorithm is is DecisionTreeClassifier\n",
    "\n",
    "Though is it not required, we will fit a model using the training data and then test the performance of the model using the testing data. Start by loading `DecisionTreeClassifier` from scikit-learn and then initializing and fitting the model. We'll start off with a model where max_depth=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "#### Nuestro segundo algoritmo es DecisionTreeClassifier\n",
    "\n",
    "Aunque no es obligatorio, ajustaremos un modelo utilizando los datos de entrenamiento y luego probaremos el rendimiento del modelo utilizando los datos de prueba. Comience cargando `DecisionTreeClassifier` desde scikit-learn y luego inicializando y ajustando el modelo. Comenzaremos con un modelo donde max_depth=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtc = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "dtc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2=dtc.predict(X_test)\n",
    "\n",
    "y_pred2[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob2=dtc.predict_proba(X_test)\n",
    "\n",
    "y_prob2[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_pred2==y_test)/y_pred2.shape[0] * 100   # acierto, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "To test your model, compute the predicted probabilities, decide 0 or 1 using a threshold of 0.5 and print the confusion matrix as well as the accuracy score (on the test set!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "Para probar su modelo, calcule las probabilidades pronosticadas, decida 0 o 1 usando un umbral de 0.5 e imprima la matriz de confusión, así como el puntaje de precisión (¡en el conjunto de prueba!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "y_prob_05=[e[1] for e in y_prob2]\n",
    "y_pred_prime=[0 if e<0.50 else 1 for e in y_prob_05] \n",
    "sum(y_pred_prime==y_test)/y_test.shape[0] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "#### We'll create another DecisionTreeClassifier model with max_depth=5. \n",
    "Initialize and fit the model below and print the confusion matrix and the accuracy score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "#### Crearemos otro modelo de DecisionTreeClassifier con max_depth=5.\n",
    "Inicialice y ajuste el modelo a continuación e imprima la matriz de confusión y la puntuación de precisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "dtc2 = DecisionTreeClassifier(random_state=0, max_depth=5)\n",
    "\n",
    "dtc2.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred3=dtc2.predict(X_test)\n",
    "\n",
    "y_pred3[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob3=dtc2.predict_proba(X_test)\n",
    "\n",
    "y_prob3[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_pred3==y_test)/y_pred3.shape[0] * 100   # acierto, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "Did you see an improvement in the confusion matrix when increasing max_depth to 5? Did you see an improvement in the accuracy score? Write your conclusions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "¿Observó una mejora en la matriz de confusión al aumentar la profundidad máxima a 5? ¿Observó una mejora en la puntuación de precisión? Escribe tus conclusiones a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your conclusions here:\n",
    "'''Observo que el valor empeora, ligeramente'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Challenge - Feature Scaling\n",
    "\n",
    "Problem-solving in machine learning is iterative. You can improve your model prediction with various techniques (there is a sweetspot for the time you spend and the improvement you receive though). Now you've completed only one iteration of ML analysis. There are more iterations you can conduct to make improvements. In order to be able to do that, you will need deeper knowledge in statistics and master more data analysis techniques. In this bootcamp, we don't have time to achieve that advanced goal. But you will make constant efforts after the bootcamp to eventually get there.\n",
    "\n",
    "However, now we do want you to learn one of the advanced techniques which is called *feature scaling*. The idea of feature scaling is to standardize/normalize the range of independent variables or features of the data. This can make the outliers more apparent so that you can remove them. This step needs to happen during Challenge 6 after you split the training and test data because you don't want to split the data again which makes it impossible to compare your results with and without feature scaling. For general concepts about feature scaling, click [here](https://en.wikipedia.org/wiki/Feature_scaling). To read deeper, click [here](https://medium.com/greyatom/why-how-and-when-to-scale-your-features-4b30ab09db5e).\n",
    "\n",
    "In the next cell, attempt to improve your model prediction accuracy by means of feature scaling. A library you can utilize is `sklearn.preprocessing.RobustScaler` ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html)). You'll use the `RobustScaler` to fit and transform your `X_train`, then transform `X_test`. You will use logistic regression to fit and predict your transformed data and obtain the accuracy score in the same way. Compare the accuracy score with your normalized data with the previous accuracy data. Is there an improvement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "es",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
